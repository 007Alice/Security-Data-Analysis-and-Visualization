{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "init_url='http://l-team.org/about-us.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_host(friend_links):\n",
    "    sur_host={}\n",
    "    fail_host={}\n",
    "    problem_host={}\n",
    "    #requests.adapters.DEFAULT_RETRIES = 5 # 增加重连次数\n",
    "    s = requests.session()\n",
    "    s.keep_alive = False\n",
    "    s.adapters.DEFAULT_RETRIES = 10\n",
    "    for key in friend_links.keys():\n",
    "        \n",
    "        try:\n",
    "            #print(key,friend_links[key],s.get(friend_links[key]))\n",
    "            if s.get(friend_links[key],timeout=10).status_code==200:\n",
    "                sur_host[key]=friend_links[key]\n",
    "            else:\n",
    "                problem_host[key]=friend_links[key]\n",
    "        except Exception as e:\n",
    "            #print(\"{}{}\".format(e,friend_links[key]))\n",
    "            fail_host[key]=friend_links[key]\n",
    "    return sur_host,problem_host,fail_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#当友链是单独页面时，从首页提取友链页面链接，再提取友链\n",
    "#存在网页解码识别不了“友情链接”问题https://www.sqlsec.com\n",
    "def from_index_search_links(url):\n",
    "    try:\n",
    "        a_url=''\n",
    "        s = requests.session()\n",
    "        s.keep_alive = False\n",
    "        s.adapters.DEFAULT_RETRIES = 10\n",
    "        result=s.get(url,timeout=10)\n",
    "        bsObj=BeautifulSoup(result.text,'html.parser')\n",
    "        a_links=bsObj.find_all('a')\n",
    "        for a_link in a_links:\n",
    "            #print(a_link)\n",
    "            #if '友链' in a_link.text or '友情链接' in a_link.text or 'FriendLink' in a_link.text or 'Friendlink' in a_link.text or 'friends' in a_link.text or 'links' in a_link.text or 'Friends' in a_link.text or 'Links' in a_link.text:\n",
    "            if [True for str in (\"友链\",\"友情链接\",\"FriendLink\",\"Friendlink\",\"friends\",\"FRIENDS\",'links','Links','Link')if str in a_link.text]:\n",
    "                if 'http' not in a_link.get('href'):\n",
    "                    a_url=url.rstrip('/')+a_link.get('href')\n",
    "                else:\n",
    "                    a_url=a_link.get('href')\n",
    "            \n",
    "        return a_url\n",
    "    except Exception as err:\n",
    "        #print(str(err))\n",
    "        pass\n",
    "\n",
    "\n",
    "def from_link_search_friend(link):\n",
    "    friend_link_url={}\n",
    "    try:    \n",
    "        s = requests.session()\n",
    "        s.keep_alive = False\n",
    "        s.adapters.DEFAULT_RETRIES = 10\n",
    "        result=s.get(link,timeout=10)\n",
    "        bsObj=BeautifulSoup(result.text,'html.parser')\n",
    "        a_urls = bsObj.find_all('a')\n",
    "        for a_url in a_urls:\n",
    "            if re.search('http(.+?)(com|net|blog|pw|cc|xyz|club|org|cn|io|info|me|im|link)(\\/)?$',a_url.get('href')) != None and a_url.get('href') not in link:\n",
    "                #print(a_url.get('href'),a_url.text)\n",
    "                if a_url.get('href')!='http://www.52bug.cn':\n",
    "                    friend_link_url[a_url.text.strip('\\n')]=(a_url.get('href').strip('/'))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(str(e))\n",
    "    #print(len(friend_link_url.keys()))\n",
    "    return friend_link_url\n",
    "\n",
    "#增加主页关键字\"链接\" or \"Links\" https://www.virzz.com\n",
    "#http://iamstudy.cnblogs.com/\n",
    "#需要增加关键字Bookmarks（https://www.melodia.pw/的section标签中）,Friendship website（http://poetichacker.com/的div标签中）\n",
    "def search_link(key,url):\n",
    "    result=[]\n",
    "    friend_link_url={}\n",
    "    try:\n",
    "        html_result=requests.get(url)\n",
    "        status=False\n",
    "        #print(url)\n",
    "        if [True for str in (\"友链\",\"友情链接\",\"FriendLink\",\"Friendlink\",\"friends\",\"FRIENDS\",'Friends')if str in html_result.text]:\n",
    "            bsObj = BeautifulSoup(html_result.text, 'html.parser')\n",
    "            searchs=['section','aside','div']\n",
    "            for search in searchs:\n",
    "                sections = bsObj.find_all(search)\n",
    "                for section in sections:\n",
    "                    if [True for str in (\"友链\",\"友情链接\",\"FriendLink\",\"Friendlink\",\"friends\",'FRIENDS','Friends')if str in section.text]:\n",
    "                        #print(\"在{}发现主页{}的友情链接\".format(search,url))\n",
    "                        a_urls = section.find_all('a')\n",
    "                        \n",
    "                        friend_link_url={}\n",
    "                        for a_url in a_urls:\n",
    "                            #print(a_url.get('href'))\n",
    "                            if a_url.get('href')!=None:\n",
    "                                ext=tldextract.extract(a_url.get('href'))\n",
    "                                main_domain=ext.domain+'.'+ext.suffix\n",
    "                                if re.search('(http)?(.+?)(com|net|blog|pw|cc|xyz|club|org|cn|io|info|me|im|win|link)(\\/)?$',a_url.get('href')) != None and main_domain not in url:\n",
    "                                    #print(a_url.text,(a_url.get('href').strip('/')))\n",
    "                                    if a_url.get('href')!='http://www.52bug.cn':\n",
    "                                        friend_link_url[a_url.text]=(a_url.get('href').strip('/'))\n",
    "                                        status=True\n",
    "                            \n",
    "                        \n",
    "                        if friend_link_url:\n",
    "                            result.append(friend_link_url)\n",
    "                if len(result)>1:\n",
    "                    #取交集       \n",
    "                    print(\"友链出现在多个同样标签中，请手工验证{}\".format(url))\n",
    "                    #print(result)\n",
    "                    #no_link.pop(key)\n",
    "                    return result[-1],key\n",
    "                    \n",
    "                if status==True:\n",
    "                    break\n",
    "            if status==False:\n",
    "                print('未在已有标签中发现{}友链，可能存在于其他标签中,请手工验证'.format(url))\n",
    "            #no_link.pop(key)\n",
    "        else:\n",
    "            print('未发现友链关键字%s' %url)\n",
    "       \n",
    "    except Exception as e:\n",
    "        #print(\"{}{}\".format(url,e))\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    return friend_link_url,key\n",
    "def find_security_blog(url_dict):\n",
    "    security_dict={}\n",
    "    no_security_dict={}\n",
    "    s = requests.session()\n",
    "    s.keep_alive = False\n",
    "    s.adapters.DEFAULT_RETRIES = 10\n",
    "    for key in url_dict.keys():\n",
    "        try:\n",
    "            if not url_dict[key].startswith(\"http\"):\n",
    "                url_dict[key]=\"http://\"+url_dict[key]\n",
    "            #print(key,url_dict[key],s.get(url_dict[key],timeout=10))\n",
    "            html=s.get(url_dict[key],timeout=10)\n",
    "            html.encoding=\"utf-8\"\n",
    "            if html.status_code==200:\n",
    "                if [True for str in (\"二进制安全\",\"安全攻防\",\"安全数据\",\"安全运营\",\"XSS\",\"信息安全协会\",\"代码审计\",\"网络安全\",\"security\",\"Penetration\",'渗透','CTF','CSRF','SQL注入','RCE','CVE','漏洞分析','安全研究')if str in html.text]:\n",
    "                    security_dict[key]=url_dict[key]\n",
    "                else:\n",
    "                    no_security_dict[key]=url_dict[key]\n",
    "                    \n",
    "            else:\n",
    "                #print(\"网页暂时打不开\")\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            #print(\"{}{}\".format(e,url_dict[key]))\n",
    "            pass\n",
    "            no_security_dict[key]=url_dict[key]  \n",
    "    return security_dict,no_security_dict\n",
    "#search_link('xjj','http://www.yqxiaojunjie.com/')\n",
    "#test\n",
    "#print(from_link_search_friend(from_index_search_links('http://balis0ng.com')))\n",
    "#print(from_index_search_links('http://le4f.net'))\n",
    "#print(from_index_search_links('http://leavesongs.com'))\n",
    "#print(from_index_search_links('http://www.venenof.com'))\n",
    "#search_link('f','http://wooyaa.me/')\n",
    "#find_security_blog({'1': 'https://www.youngxj.cn/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now {'L-team': {'http://l-team.org/about-us.html': {'tools': 'http://tool.leavesongs.com', 'Da2din9o': 'http://dazdingo.net', 'Zing': 'http://z1ng.net', 'DM_': 'http://x0day.me', 'Phiti0n': 'http://leavesongs.com', 'Le4F': 'http://le4f.net', 'Kuuki': 'http://blog.esu.im', 'Bigtang': 'http://bigtang.org', 'Chu': 'http://sh3ll.me', 'Cyrils': 'http://www.cyrils.org', 'F0r': 'http://www.f0r.info', 'Think': 'http://th1nk.info', 'math1as': 'http://www.math1as.com', 'rj1ng': 'http://rj1ng.com', 'Silver': 'https://www.iret.xyz', 'Sud0': 'http://sudalover.com', '几何_me7ell': 'http://blog.7ell.me', 'Ricky': 'https://rickyhao.com', 'Omego': 'http:/xd-a8.com', '1phan': 'http://1phan.cc', 'grt1st': 'http://www.grt1st.cn', 'Klaus': 'http://klaus.link', '天上的因幡': 'http://rabbithouse.me', 'skye': 'http://sky3.pw'}}}\n"
     ]
    }
   ],
   "source": [
    "friends={}\n",
    "result={}\n",
    "dict={}\n",
    "dict[init_url]=from_link_search_friend(init_url)\n",
    "friends['L-team']=dict\n",
    "print('now %s' %friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自身去重，与上次对比去重，得到下步需要探测的字典\n",
    "while(True):\n",
    "    next={}\n",
    "    for i in friends.keys():\n",
    "        for j in friends[i].keys():\n",
    "            for k in friends[i][j].keys():\n",
    "                next[k]=friends[i][j][k]\n",
    "    next={next[key]:key for key in next}\n",
    "    next={next[key]:key for key in next}\n",
    "    #print(next)\n",
    "    netodel=[]\n",
    "    for i in result.keys():\n",
    "        for j in result[i].keys():\n",
    "            for k in next.keys():\n",
    "                if j==next[k]:\n",
    "                    netodel.append(next[k])\n",
    "    netodel=list(set(netodel))\n",
    "    next={next[key]:key for key in next}\n",
    "    for i in netodel:\n",
    "        next.pop(i)\n",
    "    next={next[key]:key for key in next}\n",
    "    #print(next)\n",
    "    security_host,no_security_host=find_security_blog(next)\n",
    "    #初始化结果字典集\n",
    "    for i in security_host.keys():\n",
    "        #print(friends[i][j][k])\n",
    "        dict={}\n",
    "        dict[security_host[i]]=''\n",
    "        result[i]=dict\n",
    "    #print(result)\n",
    "    \n",
    "    #主机存活探测\n",
    "    \"\"\"\n",
    "    for key in friends.keys():\n",
    "        for key2 in friends[key].keys():\n",
    "            sur_host,problem_host,fail_host=get_host(friends[key][key2])\n",
    "    #注册未存活的主机和不正常的网页\n",
    "    for i in problem_host.keys():\n",
    "        for j in result[i].keys():\n",
    "            result[i][j]='404notfound'\n",
    "    for i in fail_host.keys():\n",
    "        for j in result[i].keys():\n",
    "            result[i][j]='nohost'\n",
    "    \"\"\"\n",
    "    #security_host,no_security_host=find_security_blog(next)\n",
    "    #第一遍筛选\n",
    "    no_link=security_host.copy()\n",
    "    friends={}\n",
    "    for key in security_host.keys():\n",
    "        #print('%s' %security_host[key])   \n",
    "        friend_link=from_index_search_links(security_host[key])\n",
    "        dict={}\n",
    "        if friend_link:\n",
    "            no_link.pop(key)\n",
    "            print('%s found ' %security_host[key])\n",
    "            dict[security_host[key]]=from_link_search_friend(friend_link)\n",
    "            friends[key]=dict\n",
    "\n",
    "        else:\n",
    "            print('%s not found ' %security_host[key])\n",
    "\n",
    "    #print(\"第一次搜索结果：{}\".format(len(friends)))\n",
    "    #第二遍筛选\n",
    "    needtodel=[]\n",
    "    for i in no_link.keys():\n",
    "        friend_link,key=search_link(i,no_link[i])\n",
    "        if not friend_link:\n",
    "            pass\n",
    "        else:\n",
    "            #print(friend_link,key)\n",
    "            dict={}\n",
    "            dict[no_link[i]]=friend_link\n",
    "            friends[i]=dict\n",
    "            needtodel.append(key)\n",
    "    for i in needtodel:\n",
    "        no_link.pop(i)\n",
    "    #print(\"第二次搜索结果：{}\".format(len(friends)))\n",
    "    #赋值给最终的结果集\n",
    "    for key in friends.keys():\n",
    "        result[key]=friends[key]\n",
    "    #扫尾注册\n",
    "    for i in no_link.keys():\n",
    "        dict={}\n",
    "        dict[no_link[i]]='nolink'\n",
    "        result[i]=dict\n",
    "    print(\"最终结果集为{}\".format(result))\n",
    "    print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
